# Transformers

<https://huggingface.co/docs/transformers/index>

ğŸ¤— TransformersëŠ” ìµœì‹ ì˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‰½ê²Œ ë‹¤ìš´ë¡œë“œí•˜ê³  í›ˆë ¨í•  ìˆ˜ ìˆëŠ” APIì™€ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## Tutorials

### Run inference with pipelines

<https://huggingface.co/docs/transformers/pipeline_tutorial>

pipeline()ì€ Hubì˜ ëª¨ë“  ëª¨ë¸ì„ ì–¸ì–´, ì»´í“¨í„° ë¹„ì „, ìŒì„±, ê·¸ë¦¬ê³  ë©€í‹°ëª¨ë‹¬ ì‘ì—…ì— ëŒ€í•œ ì¶”ë¡ ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

#### Pipeline usage

<https://huggingface.co/docs/transformers/pipeline_tutorial#pipeline-usage>

'pipeline()'ì€ ìë™ìœ¼ë¡œ ê¸°ë³¸ ëª¨ë¸ê³¼ ì‘ì—…ì— ëŒ€í•œ ì¶”ë¡ ì´ ê°€ëŠ¥í•œ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. 

- 01.tranformers_pipeline_basic.py: ìë™ ìŒì„± ì¸ì‹(ASR) ë˜ëŠ” ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ ì˜ˆì‹œ
  - 1. taskë§Œ ì„¤ì • í•˜ê³  ëª¨ë¸ ì§€ì • ì—†ì´ ì‚¬ìš©
  - 2. task ì—†ì´ ëª¨ë¸ ì§€ì • í•˜ê¸°: 'openai/whisper-large-v2'
  - 3. ì˜ˆì œê°€ ì•„ë‹Œ ë‹¤ë¥¸ ëª¨ë¸ ì§€ì •: 'openai/whisper-large-v3'

```bash
# ValueError: ffmpeg was not found but is required to load audio files from filename
# ìœ„ì™€ ê°™ì€ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ffmpegë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
sudo apt update
sudo apt install ffmpeg
```

##### ì§€ì› ë˜ëŠ” tasks

ì†ŒìŠ¤ ì½”ë“œë¥¼ ì°¸ì¡°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ taskë“¤ì„ ì§€ì›í•˜ëŠ” ê±¸ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<https://github.com/huggingface/transformers/blob/v4.43.3/src/transformers/pipelines/__init__.py#L552>

- `"audio-classification"`: will return a [`AudioClassificationPipeline`].
- `"automatic-speech-recognition"`: will return a [`AutomaticSpeechRecognitionPipeline`].
- `"depth-estimation"`: will return a [`DepthEstimationPipeline`].
- `"document-question-answering"`: will return a [`DocumentQuestionAnsweringPipeline`].
- `"feature-extraction"`: will return a [`FeatureExtractionPipeline`].
- `"fill-mask"`: will return a [`FillMaskPipeline`]:.
- `"image-classification"`: will return a [`ImageClassificationPipeline`].
- `"image-feature-extraction"`: will return an [`ImageFeatureExtractionPipeline`].
- `"image-segmentation"`: will return a [`ImageSegmentationPipeline`].
- `"image-to-image"`: will return a [`ImageToImagePipeline`].
- `"image-to-text"`: will return a [`ImageToTextPipeline`].
- `"mask-generation"`: will return a [`MaskGenerationPipeline`].
- `"object-detection"`: will return a [`ObjectDetectionPipeline`].
- `"question-answering"`: will return a [`QuestionAnsweringPipeline`].
- `"summarization"`: will return a [`SummarizationPipeline`].
- `"table-question-answering"`: will return a [`TableQuestionAnsweringPipeline`].
- `"text2text-generation"`: will return a [`Text2TextGenerationPipeline`].
- `"text-classification"` (alias `"sentiment-analysis"` available): will return a [`TextClassificationPipeline`].
- `"text-generation"`: will return a [`TextGenerationPipeline`]:.
- `"text-to-audio"` (alias `"text-to-speech"` available): will return a [`TextToAudioPipeline`]:.
- `"token-classification"` (alias `"ner"` available): will return a [`TokenClassificationPipeline`].
- `"translation"`: will return a [`TranslationPipeline`].
- `"translation_xx_to_yy"`: will return a [`TranslationPipeline`].
- `"video-classification"`: will return a [`VideoClassificationPipeline`].
- `"visual-question-answering"`: will return a [`VisualQuestionAnsweringPipeline`].
- `"zero-shot-classification"`: will return a [`ZeroShotClassificationPipeline`].
- `"zero-shot-image-classification"`: will return a [`ZeroShotImageClassificationPipeline`].
- `"zero-shot-audio-classification"`: will return a [`ZeroShotAudioClassificationPipeline`].
- `"zero-shot-object-detection"`: will return a [`ZeroShotObjectDetectionPipeline`].

#### Parameters

<https://huggingface.co/docs/transformers/pipeline_tutorial#parameters>

'pipeline()'ì€ ë§ì€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì¼ë¶€ëŠ” íŠ¹ì • ì‘ì—…ì— ê´€ë ¨ë˜ê³ , ì¼ë¶€ëŠ” ëª¨ë“  íŒŒì´í”„ë¼ì¸ì— ê³µí†µì ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì›í•˜ëŠ” ê³³ ì–´ë””ì—ì„œë‚˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> Device, batch_size

- 02.tranformers_pipeline_params_device.py: pipeline ë§¤ê°œë³€ìˆ˜(device) ì„¤ì • ì˜ˆì‹œ
  - 1. 'device' - '0': GPU, '-1': CPU
  - 2. 'device_map' - ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì–´ë–»ê²Œ ë¡œë“œí•˜ê³  ì €ì¥í• ì§€ ìë™ìœ¼ë¡œ ê²°ì •
  - 3. 'batch_size' - 2ë¡œ ì„¤ì • => ì˜¤ë¥˜ ë°œìƒ(ì›ì¸ ì°¾ì•„ì•¼ í•¨), 1ë¡œ ì„¤ì • ì‹œ ì‘ë™
    - ê¸°ë³¸ì ìœ¼ë¡œ pipelineì€ ë°°ì¹˜ ì¶”ë¡ ì„ í•˜ì§€ ì•Šì§€ë§Œ ì‚¬ìš© í•  ê²½ìš°ì˜ ì˜ˆì‹œ

- 03.tranformers_pipeline_params_task_spectific.py: taskì— ë”°ë¥¸ ë§¤ê°œë³€ìˆ˜ ì„¤ì • ì˜ˆì‹œ
  - 1. 'return_timestamps' - ë°œìŒëœ ì‹œê°„
  - 2. 'chunk_length_s' - ê¸´ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì‘ì€ ì²­í¬(chunk)ë¡œ ë‚˜ëˆ„ë©° ê° ì²­í¬ì˜ ê¸¸ì´ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ì§€ì •

#### Using pipelines on a dataset

<https://huggingface.co/docs/transformers/pipeline_tutorial#using-pipelines-on-a-dataset>

pipelineì€ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì¶”ë¡ ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë©° ê¶Œì¥ë˜ì–´ ì§€ëŠ” ë°©ë²•ì€ ë°˜ë³µìë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

- 04.tranformers_pipeline_dataset.py: Dataset ì‚¬ìš© ì˜ˆì‹œ
  - 1. iterator(yield ì‚¬ìš©)ë¥¼ ì‚¬ìš©í•œ Dataset ì‚¬ìš©
  - 2. Hugging Faceì˜ Datasets ì‚¬ìš© - hf-internal-testing/librispeech_asr_dummy
    - ê²°ê³¼: ê²°ê³¼ê°€ textë¡œ ë‚˜ì˜¤ì§€ ì•ŠìŒ
      - hf-internal-testing/librispeech_asr_dummyì´ ëª¨ë¸ ëª©ë¡ì—ì„œ ê²€ìƒ‰ ë˜ì§€ ì•ŠìŒ
      - hf-internal-testing ì—ì„œ testìš© ëª¨ë¸ ì¸ ë“¯    
    - ì• ì½”ë“œì˜ ëª¨ë¸ë¡œ ë³€ê²½(openai/whisper-large-v2) -> ì •ìƒ ì‘ë™
  - 3. ë‹¤ë¥¸ Datasets ì‚¬ìš© - PolyAI/minds14
    - https://huggingface.co/docs/transformers/quicktour#pipeline

#### Using pipelines for a webserver

<https://huggingface.co/docs/transformers/pipeline_tutorial#using-pipelines-for-a-webserver>

pass

#### Vision pipeline

<https://huggingface.co/docs/transformers/pipeline_tutorial#vision-pipeline>

Vision ì‘ì—…ì— 'pipeline()'ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì‹¤ì œë¡œ ê±°ì˜ ë™ì¼

- 05.tranformers_pipeline_vision.py: Vision ì‘ì—…ì˜ pipeline ì‚¬ìš© ì˜ˆì‹œ

#### Text pipeline

<https://huggingface.co/docs/transformers/pipeline_tutorial#text-pipeline>

- 06.tranformers_pipeline_text.py: Text ì‘ì—…ì˜ pipeline ì‚¬ìš© ì˜ˆì‹œ

#### Multimodal pipeline

<https://huggingface.co/docs/transformers/pipeline_tutorial#multimodal-pipeline>

'pipeline()'ì€ ì—¬ëŸ¬ ëª¨ë‹¬ë¦¬í‹°(modality)ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ì™€ ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥ ìœ¼ë¡œ ë°›ëŠ” pipelineì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

- 07.tranformers_pipeline_multimodal.py: Multimodal ì‘ì—…ì˜ pipeline ì‚¬ìš© ì˜ˆì‹œ
  - ì˜¤ë¥˜ ë°œìƒ: AttributeError: 'list' object has no attribute 'numpy'
  - ì›ì¸ì„ ì°¾ì•„ì•¼ í•¨ => pip install --upgrade transformers => í•´ê²° ë¨

```bash
# ì„¤ì¹˜ í•„ìš”
sudo apt install -y tesseract-ocr
pip install pytesseract
```

#### Using pipeline on large models with ğŸ¤— accelerate

<https://huggingface.co/docs/transformers/pipeline_tutorial#using-pipeline-on-large-models-with--accelerate->

Hugging Faceì˜ accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ ëŒ€ê·œëª¨ ëª¨ë¸ì„ ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- 08.tranformers_pipeline_accelerate.py: accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì˜ˆì‹œ
  - 1.3B íŒŒë¼ë¯¸í„° ë²„ì „ì˜ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸

#### Creating web demos from pipelines with gradio

<https://huggingface.co/docs/transformers/pipeline_tutorial#creating-web-demos-from-pipelines-with-gradio>

- 09.tranformers_pipeline_gradio.py: gradioë¥¼ ì´ìš©í•œ ì›¹ë°ì½” ìƒì„± ì˜ˆì‹œ
  - <http://127.0.0.1:7860/>

#### ì¶”ê°€ ì—°ìŠµ

ìœ„ì˜ tutorialì„ ì°¸ê³ í•˜ì—¬ ì¶”ê°€ ì—°ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.

##### text-classification

<https://huggingface.co/docs/transformers/main_classes/pipelines>

- 10.tranformers_pipeline_text_classification_01.py: Text Classification ì˜ˆì‹œ
  - 1. taskë§Œ ì„¤ì • í•˜ê³  ëª¨ë¸ ì§€ì • ì—†ì´ ì‚¬ìš©
  - 2. task ì—†ì´ ëª¨ë¸ ì§€ì • í•˜ê¸°: 'FacebookAI/roberta-large-mnli'
    - ë¶„ë¥˜ ë²”ì£¼ê°€ ë‹¤ë¥¸ ëª¨ë¸: ì˜ˆì œê°€ ì•„ë‹Œ ë‹¤ë¥¸ ë¬¸ì¥ ì…ë ¥
  - 3. ì˜ˆì œê°€ ì•„ë‹Œ ë‹¤ë¥¸ ëª¨ë¸ ì§€ì •: 'BAAI/bge-reranker-v2-m3'
    - ë¶„ë¥˜ ë²”ì£¼ê°€ ë‹¤ë¥¸ ëª¨ë¸: ì˜ˆì œê°€ ì•„ë‹Œ ë‹¤ë¥¸ ë¬¸ì¥ ì…ë ¥
  - 4. í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ ëª¨ë¸: 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis'
    - ë¶„ë¥˜ ê²°ê³¼ê°€ ë™ì¼í•œ ëª¨ë¸ ê²€ìƒ‰: ê²°ê³¼ê°€ ì˜ˆìƒ ëŒ€ë¡œ ë‚˜ì˜¤ì§€ ì•ŠìŒ(ê¸ˆìœµ ë°ì´í„°ë¥¼ í•™ìŠµ í•œ ëª¨ë¸) -> ë¬¸ì¥ ë³€ê²½
     

##### text-generation

- 11.transformers_pipeline_text_generation_01.py: ìµœì‹  ëª¨ë¸ ì‚¬ìš©
  - meta-llama/Meta-Llama-3.1-8B-Instruct: ëª¨ë¸ì˜ ì ‘ê·¼ ì‹œ ì¸ì¦ í•„ìš”
    - ì…ë ¥ í•­ëª©: 'First Name', 'Last Name', 'Data of birth', 'Country', 'Affitiation', 'Job title'
    - 'ë¼ì´ì„ ìŠ¤ ë™ì˜', 'ê°œì¸ì •ë³´ ì²˜ë¦¬ ë™ì˜'
    - ëª¨ë¸ ì†Œìœ ìì˜ ìŠ¹ì¸ í•„ìš”
  - Hugging Faceì˜ Access Tokens
    - 'Profile'(ìš°ì¸¡ ìƒë‹¨) -> 'Settings' -><'Access Tokens' -> '+Create new token'
    - Token type: Write, Token name: xxx -> 'Create token' -> Copy
  - Hugging Face login ì½”ë“œ ì¶”ê°€ - Access Token í•„ìš”
  - CUDA out of memory
    - ìš°ì„  ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼. 
    - torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 119.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 23.24 GiB is allocated by PyTorch, and 1.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

### Write portable code with AutoClass

<https://huggingface.co/docs/transformers/autoclass_tutorial>

AutoClassëŠ” ì£¼ì–´ì§„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì˜¬ë°”ë¥¸ ì•„í‚¤í…ì²˜ë¥¼ ìë™ìœ¼ë¡œ ì¶”ë¡ í•˜ê³  ë¡œë“œí•©ë‹ˆë‹¤. from_pretrained() ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ì–´ë–¤ ì•„í‚¤í…ì²˜ì˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë„ ë¹ ë¥´ê²Œ ë¡œë“œí•  ìˆ˜ ìˆì–´, ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ëŠ” ë° ì‹œê°„ê³¼ ë¦¬ì†ŒìŠ¤ë¥¼ ë“¤ì¼ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì²´í¬í¬ì¸íŠ¸ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë©´, í•œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‘ë™í•˜ëŠ” ì½”ë“œëŠ” ì•„í‚¤í…ì²˜ê°€ ë‹¤ë¥´ë”ë¼ë„ ìœ ì‚¬í•œ ì‘ì—…ì„ ìœ„í•´ í›ˆë ¨ëœ ë‹¤ë¥¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œë„ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤.

- Architecture (ì•„í‚¤í…ì²˜): ì•„í‚¤í…ì²˜ëŠ” ëª¨ë¸ì˜ êµ¬ì¡°ë‚˜ "ë¼ˆëŒ€"ë¥¼ ì˜ë¯¸
  - ì˜ˆë¥¼ ë“¤ì–´, BERT, GPT, ResNet ë“±ì´ ê°ê° ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.
- Checkpoint (ì²´í¬í¬ì¸íŠ¸): íŠ¹ì • ì•„í‚¤í…ì²˜ì— ëŒ€í•´ í•™ìŠµëœ ê°€ì¤‘ì¹˜(weights)ì˜ ì§‘í•©
  - ì˜ˆë¥¼ ë“¤ì–´, 'google-bert/bert-base-uncased'ëŠ” BERT ì•„í‚¤í…ì²˜ì˜ í•œ ì²´í¬í¬ì¸íŠ¸
- Model (ëª¨ë¸): ì•„í‚¤í…ì²˜ë‚˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ì§€ì¹­í•  ìˆ˜ ìˆì–´ ë¬¸ë§¥ì— ë”°ë¼ ëª¨ë¸ì´ ì•„í‚¤í…ì²˜ë¥¼ ì˜ë¯¸í•  ìˆ˜ë„ ìˆê³ , íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ë¥¼ ì˜ë¯¸í•  ìˆ˜ë„ ìˆìŒ
  - ì˜ˆë¥¼ ë“¤ì–´, "BERT ëª¨ë¸"ì´ë¼ê³  í•  ë•ŒëŠ” BERT ì•„í‚¤í…ì²˜ë¥¼ ì˜ë¯¸í•  ìˆ˜ ìˆì§€ë§Œ, "ì‚¬ì „ í›ˆë ¨ëœ BERT ëª¨ë¸ì„ ë¡œë“œí–ˆë‹¤"ë¼ê³  í•˜ë©´ íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ë¥¼ ì˜ë¯¸í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

#### AutoTokenizer

ê±°ì˜ ëª¨ë“  NLP ì‘ì—…ì€ í† í¬ë‚˜ì´ì €ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ì…ë ¥ì„ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

- 12.transformers_autoclass_autotokenizer.py: 'AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")'

#### AutoImageProcessor

ë¹„ì „ ì‘ì—…ì˜ ê²½ìš°, ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” ì´ë¯¸ì§€ë¥¼ ì˜¬ë°”ë¥¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

- 13.transformers_autoclass_autoimageprocessor.py: 'AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")'

#### AutoBackbone

AutoBackboneì€ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ë°±ë³¸ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë°±ë³¸ì˜ ë‹¤ì–‘í•œ ë‹¨ê³„ì—ì„œ íŠ¹ì§• ë§µì„ ì–»ì„ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

- 14.transformers_autoclass_autobackbone.py: 'AutoBackbone.from_pretrained("microsoft/swin-tiny-patch4-window7-224", out_indices=(1,))'
  - 'out_indices': íŠ¹ì§• ë§µì„ ì–»ê³ ì í•˜ëŠ” ë ˆì´ì–´ì˜ ì¸ë±ìŠ¤
  - 'out_features': íŠ¹ì§• ë§µì„ ì–»ê³ ì í•˜ëŠ” ë ˆì´ì–´ì˜ ì´ë¦„

#### AutoFeatureExtractor

ì˜¤ë””ì˜¤ ì‘ì—…ì˜ ê²½ìš°, íŠ¹ì„± ì¶”ì¶œê¸°ê°€ ì˜¤ë””ì˜¤ ì‹ í˜¸ë¥¼ ì˜¬ë°”ë¥¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

- 15.transformers_autoclass_autofeatureextractor.py

#### AutoModel

Pytorch: AutoModelFor í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©´ ì£¼ì–´ì§„ ì‘ì—…ì— ëŒ€í•´ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆì œë§Œ ë´ì„  ì´í•´ê°€ ì–´ë µìŠµë‹ˆë‹¤.

- 16.transformers_autoclass_automodelfor.py 
  - DistilBERTëŠ” BERT(Bidirectional Encoder Representations from Transformers)ì˜ ê²½ëŸ‰í™” ë²„ì „ì…ë‹ˆë‹¤. ì›ë˜ BERT ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê±°ì˜ ìœ ì§€í•˜ë©´ì„œë„ í¬ê¸°ì™€ ì†ë„ ë©´ì—ì„œ ê°œì„ ëœ ëª¨ë¸ì…ë‹ˆë‹¤.
  - "base": ê¸°ë³¸ í¬ê¸°ì˜ ëª¨ë¸ì„ ì˜ë¯¸í•©ë‹ˆë‹¤ (ëŒ€í˜• ëª¨ë¸ë„ ìˆìŠµë‹ˆë‹¤).
  - "uncased": í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

##### Quick tour ì¤‘ ì¼ë¶€

<https://huggingface.co/docs/transformers/quicktour#autoclass>

AutoModelForSequenceClassificationê³¼ AutoTokenizer í´ë˜ìŠ¤ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í•¨ê»˜ ì‘ë™í•˜ì—¬ ìœ„ì—ì„œ ì‚¬ìš©í•œ **pipeline()**ì„ êµ¬ë™í•©ë‹ˆë‹¤. AutoClassëŠ” ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ë¥¼ ê·¸ ì´ë¦„ì´ë‚˜ ê²½ë¡œë¡œë¶€í„° ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ë‹¨ì¶• ë°©ë²•ì…ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì€ ë‹¨ì§€ ì‘ì—…ì— ì í•©í•œ AutoClassì™€ ê·¸ì— ì—°ê´€ëœ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

> AutoTokenizer

í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ«ì ë°°ì—´ë¡œ ì „ì²˜ë¦¬í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. í† í°í™” ê³¼ì •ì„ ê´€ë¦¬í•˜ëŠ” ì—¬ëŸ¬ ê·œì¹™ì´ ìˆìœ¼ë©°, ì—¬ê¸°ì—ëŠ” ë‹¨ì–´ë¥¼ ì–´ë–»ê²Œ ë¶„í• í• ì§€, ì–´ëŠ ìˆ˜ì¤€ì—ì„œ ë‹¨ì–´ë¥¼ ë¶„í• í•´ì•¼ í• ì§€ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤. ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ëœ ê²ƒê³¼ ë™ì¼í•œ í† í°í™” ê·œì¹™ì„ ì‚¬ìš©í•˜ê³  ìˆìŒì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ê°™ì€ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.

í† í¬ë‚˜ì´ì €ê°€ ë°˜í™˜í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ì—ëŠ” ë‹¤ìŒì´ í¬í•¨ë©ë‹ˆë‹¤:
- input_ids: í† í°ì˜ ìˆ˜ì¹˜ì  í‘œí˜„ì…ë‹ˆë‹¤.
- attention_mask: ì–´ë–¤ í† í°ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì—¬ì•¼ í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

- 17.transformers_autoclass_autotokenizer.py
  - nlptown/bert-base-multilingual-uncased-sentiment
    - nlptown: ì´ ëª¨ë¸ì„ ê°œë°œí•˜ê³  ê³µê°œí•œ ì¡°ì§ ë˜ëŠ” ì‚¬ìš©ìì˜ ì´ë¦„
    - bert: ì´ ëª¨ë¸ì˜ ê¸°ë³¸ êµ¬ì¡°ê°€ BERT ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©
    - base: BERTì˜ ê¸°ë³¸ í¬ê¸° ë²„ì „ -'small'ë³´ë‹¤ í¬ê³  'large'ë³´ë‹¤ ì‘ì€ ëª¨ë¸
    - multilingual: ë‹¤êµ­ì–´ ì§€ì›
    - uncased: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ
    - sentiment: ê°ì„± ë¶„ì„ - ê¸ì •/ë¶€ì •

> AutoModel

TransformersëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë¡œë“œí•˜ëŠ” ê°„ë‹¨í•˜ê³  í†µì¼ëœ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” AutoTokenizerë¥¼ ë¡œë“œí•˜ëŠ” ê²ƒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ AutoModelì„ ë¡œë“œí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ìœ ì¼í•œ ì°¨ì´ì ì€ ì‘ì—…ì— ë§ëŠ” ì˜¬ë°”ë¥¸ AutoModelì„ ì„ íƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸(ë˜ëŠ” ì‹œí€€ìŠ¤) ë¶„ë¥˜ì˜ ê²½ìš°, AutoModelForSequenceClassificationì„ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.

- 18.transformers_autoclass_automodelfor.py
